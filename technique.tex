\section{Background and Terminology}

\section{Evaluation for Long-Running Campaigns}

\subsection{Evaluating Oracle Changes}

Oracle~\cite{Barr2015,Staats:2011:PTO:1985793.1985847} changes are those modifications to a fuzz target or fuzz harness whose purpose is to \emph{increase the set of executions that are deemed test failures}; in fuzzing, this basically means changes that \emph{increase the number of crashing inputs.}  Oracle changes range from adding a single, highly specific, assertion inside SUT code, to extensive rewriting of a fuzz harness, potentially changing the set of inputs that it is capable of generating and adding an expensive and complex check for correct execution after the change.   An example of the latter is transforming a normal fuzz harness into a \emph{differential}\cite{Differential} harness, where inputs are applied to both the SUT and a reference implementation, and their behavior is compared (with certain differences considered as test failures).

At first glance, it may seem that no evaluation is needed for oracle changes: increasing the set of failing runs is always good, unless the change introduces a large number of false positives.  If a change introduces few false positives, and many true positives, it is good, and no special method is needed to observe that a change has produced so many false positives it is making the fuzzing campaign less useful.  The problem is that an oracle change may have costs other than false positives.  In particular, some oracle changes greatly reduce fuzzing throughput.  Differential oracles, while very powerful, often at least double the time to run a particular input, and may be more costly than that, if the reference implementation is slow or the comparison of behaviors is complex.  If an oracle change detects few bugs, and greatly reduces the ability of the fuzzer to explore behaviors, it may be harmful, or at least best limited to occasional runs over a corpus produced by more efficient methods.