\section{Introduction}

Software, of course, changes over time; this means that even an ostensibly bug-free program, if important enough to matter, must be supported by a regime of tests that determine if future changes induce new bugs.  For critically important software facing not only ``natural'' disasters of unexpected input sequences, but human adversaries aiming to undermine the system, the level of testing required to protect software is extremely high indeed.  \emph{Fuzzing}~\cite{ArtSciFuzz}, the subjection of a program to randomly generated (and, usually, intelligently adapted based on code coverage and other signals) inputs, is one of the most important tools for assuring that such programs are secure and reliable.  To this end, the most critical software systems are often subjected to computationally vigorous long-running \emph{fuzzing campaigns}.  E.g., Google's oss-fuzz service~\cite{ossfuzz} provides continuous fuzzing of over a thousand critical open-source software systems, and has detected more than 10,000 vulnerabilities and 30,000 bugs, to date.

Fuzzing efforts, too, change over time.  When new functionality is introduced to a software system, new tests must often be added to the code that fuzzes a system, especially when fuzzing is implemented as a series of independent fuzzers targeting particular functionalities, rather than a single monolithic subjection of a whole program to fuzz inputs.  Fuzzing engineers also introduce changes to how fuzzing is performed, reconfiguring the generation and adaptation of inputs to reflect improvements to the art and science of fuzzing, and to avoid \emph{saturation}.  Saturation~\cite{SatFuzz,ReachCov} is an effect where, even if in theory a particular fuzzing approach is highly effective, it over time discovers all interesting program inputs for which \emph{this particular approach} is highly effective, and can be improved upon even by less (theoretically) effective approaches that have an easier time reaching new behaviors.  Furthermore, fuzzing engineers often try to improve the power of a fuzzing effort by adding new checks for correctness to a fuzzing effort, adding to the set of behaviors that are deemed to have exposed a bug or vulnerability in the system.

Like software development, fuzzing development is subject to human error; an intended improvement to a fuzzing configuration can actually degrade the ability to detect bugs.  Furthermore, fuzzing is subject to surprises that are not rooted in human error: a change that should, so far as human understanding is concerned, be an improvement may, for reasons beyond human understanding (given the complexity of fuzzing and our current understanding of software systems) turn out to be harmful instead of beneficial.  Thus, like developers of production code, developers of fuzzing code and configurations need a way to determine if their efforts are beneficial or harmful to the task at hand.  Fuzzing engineers need ways to evaluate changes in fuzzing effectiveness, just as software developers need ways to check for bugs in the software they are developing.

How do fuzzing engineering go about this problem?  The evaluation of fuzzers is a topic of substantial study in the literature, ranging from summations of widely agreed-upon conventions~\cite{Hicks18} to highly speculative approches~\cite{FuzzAppeal}.  In general, these evaluations are all based on the idea of comparing fuzzers by computing statistics over evaluation measures collected for multiple runs of each fuzzer over a set of benchmarks.  Evaluation measures are typically limited to code coverage and counts of distinct bugs detected (the latter often somewhat approximate~\cite{PLDI13}).  These evaluations almost always ``start from scratch'' and determine, essentially, how well fuzzers can fuzz a previously un-fuzzed program in a period of 24-48 hours.

  This approach may be useful for evaluating the promise of novel fuzzing algorithms in academic research papers, but it has significant limitations in evaluating proposed changes to long-running fuzzing campaigns:

  \begin{enumerate}
  \item First, and most importantly, starting from scratch (an empty corpus of previously generated interesting inputs) is inappropriate.  Engineers do not want to know how well fuzzing would go if they threw away the results of many thousands of hours of compute time spent exploring a program.  Starting from scratch, for example, completely eliminates the effects of saturation.  Appropriate evaluations of real-world long-running fuzzing campaigns must generally start from the existing corpus of discovered inputs.
  \item Given this fundamental difference, basing evaluations on incremental code coverage is almost guaranteed to be impossible, if the evaluation time is limited to standard 24 hour runs, or even to 48 hour runs.  In fact, for stable programs that have been long-fuzzed in a setting such as OSS-Fuzz, reaching new coverage may require weeks of fuzzing, or coverage may essentially be total, especially for critical parts of the program (e.g., the Bitcoin Core fuzzing reached essentially total coverage of transaction validation years ago~\cite{Lacunae}).
  \item Similarly, expecting new methods (or old ones) to find bugs in programs that have been fuzzed for many thousands of hours in a 24 or 48 hour time period is highly unrealistic.  In practice, both coverage and bug counts, incremental from a corpus of inputs collected over what may be years of fuzzing, will be almost always be \emph{zero} for all methods.
\end{enumerate}

Given these problems with the standard evaluation methods, how can engineers trying to improve a long-running fuzzing effort evaluate their proposed changes?   Fundamentally, the problem is that there just are not enough coverage targets and real bugs to form a basis for decision-making.

\subsection{Evaulation of Fuzzer Changes via Program Mutants}

Mutation testing is, essentially, the evaluation of testing efforts by the injection of ``fake bugs.''  While introduced in the late 1970s~\cite{demillo1978hints,mathur2012foundations,demillo1978hints}, it has recently been aggressively adopted by major software developers, including Google~\cite{GoogleMut}, Meta~\cite{BellerFacebookMutation}, and
  Amazon~\cite{AmazonMut}, as the technology for mutation testing has improved and sufficient computing power to make it practical has become available.  Mutation testing relies on mutation generation, the production of small random changes in a program; such changes are expected to usually introduce bugs in the program (given the original code was correct).  Mutation generation is available for essentially all even moderately-widely-used programming languages~\cite{UMSyntax}.

Mutation testing provides a way to get around the problem that there are too few coverage targets and too few real, undetected, bugs in software to enable effective evaluation of changes to real-world fuzzing campaigns.  Mutations provides a source of large numbers of program behaviors that are of interest (in that the represent \emph{potential} bugs in software).  While most program mutants are likely to be of little interest (easily detected by existing tests/fuzzing corpus), the large number of mutants for even a small part of a program (on average, perhaps as many as 2 mutants per LOC) means that many mutants that represent hard-to-detect bugs are likely to exist for any program of substantial size.
  
In order to effectively evaluate changes to long-running fuzzing campaigns, we make a core distinction between two kinds of changes.  The first kind, \emph{oracle changes}, can be evaluated using what are essentially off-the-shelf mutation testing techniques, properly configured.  The second type of changes, \emph{fuzzing strategy changes}, however, require a set of complex changes to standard mutation testing practice in order to evaluate changes in a time-effective manner.

\subsection{Evaluating Oracle Changes}

Oracle~\cite{Barr2015,Staats:2011:PTO:1985793.1985847} changes are those modifications to a fuzz target or fuzz harness whose purpose is to \emph{increase the set of executions that are deemed test failures}; in fuzzing, this basically means changes that \emph{increase the number of crashing inputs.}  Oracle changes range from adding a single, highly specific, assertion inside SUT code, to extensive rewriting of a fuzz harness, potentially changing the set of inputs that it is capable of generating and adding an expensive and complex check for correct execution after the change.   An example of the latter is transforming a normal fuzz harness into a \emph{differential}\cite{Differential} harness, where inputs are applied to both the SUT and a reference implementation, and their behavior is compared (with certain differences considered as test failures).

At first glance, it may seem that no evaluation is needed for oracle changes: increasing the set of failing runs is always good, unless the change introduces a large number of false positives.  If a change introduces few false positives, and many true positives, it is good, and no special method is needed to observe that a change has produced so many false positives it is making the fuzzing campaign less useful.  The problem is that an oracle change may have costs other than false positives.  In particular, some oracle changes greatly reduce fuzzing throughput.  Differential oracles, while very powerful, often at least double the time to run a particular input, and may be more costly than that, if the reference implementation is slow or the comparison of behaviors is complex.  If an oracle change detects few bugs, and greatly reduces the ability of the fuzzer to explore behaviors, it may be harmful, or at least best limited to occasional runs over a corpus produced by more efficient methods.